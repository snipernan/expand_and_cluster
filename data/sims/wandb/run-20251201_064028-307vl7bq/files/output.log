Generating dataset with teacher...
============================================================================================================================================
Reconstruction
--------------------------------------------------------------------------------------------------------------------------------------------
Reconstruction Location: /root/expand_and_cluster/data/sims/ec_3c2502264b/seed_-1/main/clustering_cf9820f28b
--------------------------------------------------------------------------------------------------------------------------------------------

Generating dataset with teacher...
--------------------------------------------------------------------------------------------------------------------------------------------
Layer 0
--------------------------------------------------------------------------------------------------------------------------------------------

computing pairwise L2 distances... Done!
Finding tree cut threshold
Extracting clusters
Found 30 of size bigger than 2.0.
removing 0/30 unaligned clusters
Collapsing 30 clusters.
producing dendrogram...
producing simmat...
Collapsing based on best student neuron 'cluster number -> student rank':
#0->1, #3->3, #6->1, #10->5, #11->2, #12->3, #14->1, #15->1, #16->1, #17->1, #19->4, #22->1, #23->1, #25->2, #29->3,
Final layer size: 15

train	ep 00000	it 000	loss 1.217e+01	ex 10000	time 0.00s
train	ep 00100	it 000	loss 1.860e-01	ex 10000	time 6.91s
train	ep 00200	it 000	loss 6.018e-02	ex 10000	time 6.80s
train	ep 00300	it 000	loss 3.068e-02	ex 10000	time 6.75s
train	ep 00400	it 000	loss 1.744e-02	ex 10000	time 6.83s
train	ep 00500	it 000	loss 9.996e-03	ex 10000	time 6.80s
train	ep 00600	it 000	loss 6.421e-03	ex 10000	time 6.78s
train	ep 00700	it 000	loss 4.364e-03	ex 10000	time 6.82s
train	ep 00800	it 000	loss 3.548e-03	ex 10000	time 6.87s
train	ep 00900	it 000	loss 2.893e-03	ex 10000	time 6.75s
train	ep 01000	it 000	loss 2.203e-03	ex 10000	time 6.86s
train	ep 01100	it 000	loss 1.866e-03	ex 10000	time 6.83s
train	ep 01200	it 000	loss 1.632e-03	ex 10000	time 6.69s
train	ep 01300	it 000	loss 1.446e-03	ex 10000	time 6.82s
train	ep 01400	it 000	loss 1.302e-03	ex 10000	time 6.84s
train	ep 01500	it 000	loss 1.106e-03	ex 10000	time 6.74s
train	ep 01600	it 000	loss 9.311e-04	ex 10000	time 6.80s
train	ep 01700	it 000	loss 7.861e-04	ex 10000	time 6.82s
train	ep 01800	it 000	loss 6.900e-04	ex 10000	time 6.73s
train	ep 01900	it 000	loss 6.152e-04	ex 10000	time 6.80s
train	ep 02000	it 000	loss 5.571e-04	ex 10000	time 6.81s
train	ep 02100	it 000	loss 5.010e-04	ex 10000	time 6.73s
train	ep 02200	it 000	loss 4.517e-04	ex 10000	time 6.83s
train	ep 02300	it 000	loss 4.085e-04	ex 10000	time 6.81s
train	ep 02400	it 000	loss 3.799e-04	ex 10000	time 6.71s
train	ep 02500	it 000	loss 3.577e-04	ex 10000	time 6.82s
train	ep 02600	it 000	loss 3.358e-04	ex 10000	time 6.85s
train	ep 02700	it 000	loss 3.188e-04	ex 10000	time 6.73s
train	ep 02800	it 000	loss 3.083e-04	ex 10000	time 6.82s
train	ep 02900	it 000	loss 2.930e-04	ex 10000	time 6.80s
train	ep 03000	it 000	loss 2.804e-04	ex 10000	time 6.75s
train	ep 03100	it 000	loss 2.708e-04	ex 10000	time 6.81s
train	ep 03200	it 000	loss 2.640e-04	ex 10000	time 6.85s
train	ep 03300	it 000	loss 2.535e-04	ex 10000	time 6.74s
train	ep 03400	it 000	loss 2.427e-04	ex 10000	time 6.78s
train	ep 03500	it 000	loss 2.346e-04	ex 10000	time 6.78s
train	ep 03600	it 000	loss 2.270e-04	ex 10000	time 6.71s
train	ep 03700	it 000	loss 2.197e-04	ex 10000	time 6.81s
train	ep 03800	it 000	loss 2.116e-04	ex 10000	time 6.80s
train	ep 03900	it 000	loss 2.045e-04	ex 10000	time 6.75s
train	ep 04000	it 000	loss 1.997e-04	ex 10000	time 6.84s
train	ep 04100	it 000	loss 1.954e-04	ex 10000	time 6.84s
train	ep 04200	it 000	loss 1.913e-04	ex 10000	time 6.76s
train	ep 04300	it 000	loss 1.893e-04	ex 10000	time 6.83s
train	ep 04400	it 000	loss 1.871e-04	ex 10000	time 6.82s
train	ep 04500	it 000	loss 1.830e-04	ex 10000	time 6.75s
train	ep 04600	it 000	loss 1.808e-04	ex 10000	time 6.86s
train	ep 04700	it 000	loss 1.798e-04	ex 10000	time 6.86s
train	ep 04800	it 000	loss 1.772e-04	ex 10000	time 6.80s
train	ep 04900	it 000	loss 1.767e-04	ex 10000	time 6.89s
train	ep 05000	it 000	loss 1.739e-04	ex 10000	time 6.91s
--------------------------------------------------------------------------------------------------------------------------------------------
Layer 1
--------------------------------------------------------------------------------------------------------------------------------------------

computing pairwise L2 distances... Done!
Finding tree cut threshold
Extracting clusters
Found 58 of size bigger than 2.0.
removing 55/58 unaligned clusters
Collapsing 3 clusters.
producing dendrogram...
producing simmat...
Collapsing based on best student neuron 'cluster number -> student rank':
#1->1,
Final layer size: 1
Traceback (most recent call last):
  File "/root/expand_and_cluster/EC.py", line 76, in <module>
    main()
  File "/root/expand_and_cluster/EC.py", line 72, in main
    platform.run_job(runner_registry.get(runner_name).create_from_args(args).run)
  File "/root/expand_and_cluster/platforms/base.py", line 130, in run_job
    f()
  File "/root/expand_and_cluster/extraction/runner.py", line 79, in run
    expand_and_cluster.reconstruct(students, losses, self.desc.extraction_path(self.global_seed),
  File "/root/expand_and_cluster/extraction/expand_and_cluster.py", line 227, in reconstruct
    y_frozen.append(frozen_students(examples))
  File "/opt/miniconda3/envs/ec_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/expand_and_cluster/models/students_custom.py", line 66, in forward
    x = layer(x)
  File "/opt/miniconda3/envs/ec_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/expand_and_cluster/models/students_custom.py", line 37, in forward
    return self.act_fun(torch.einsum('bin,ihn->bhn', x, self.fc) +
  File "/opt/miniconda3/envs/ec_env/lib/python3.10/site-packages/torch/functional.py", line 378, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
RuntimeError: einsum(): subscript i has size 12 for operand 1 which does not broadcast with previously seen size 15
