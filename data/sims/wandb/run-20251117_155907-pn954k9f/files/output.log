Generating dataset with teacher...
============================================================================================================================================
Reconstruction
--------------------------------------------------------------------------------------------------------------------------------------------
Reconstruction Location: /home/alvin/expand-and-cluster/data/sims/ec_6830710519/seed_-1/main/clustering_851c1b8aed
--------------------------------------------------------------------------------------------------------------------------------------------

Generating dataset with teacher...
--------------------------------------------------------------------------------------------------------------------------------------------
Layer 0
--------------------------------------------------------------------------------------------------------------------------------------------

computing pairwise L2 distances... Done!
Finding tree cut threshold
Extracting clusters
Found 6 of size bigger than 12.0.
removing 0/6 unaligned clusters
Collapsing 6 clusters.
producing dendrogram...
producing simmat...
Collapsing based on best student neuron 'cluster number -> student rank':
#0->1, #2->1, #3->1, #4->1, #5->3,
Final layer size: 5
Traceback (most recent call last):
  File "/home/alvin/expand-and-cluster/EC.py", line 76, in <module>
    main()
  File "/home/alvin/expand-and-cluster/EC.py", line 72, in main
    platform.run_job(runner_registry.get(runner_name).create_from_args(args).run)
  File "/home/alvin/expand-and-cluster/platforms/base.py", line 130, in run_job
    f()
  File "/home/alvin/expand-and-cluster/extraction/runner.py", line 79, in run
    expand_and_cluster.reconstruct(students, losses, self.desc.extraction_path(self.global_seed),
  File "/home/alvin/expand-and-cluster/extraction/expand_and_cluster.py", line 231, in reconstruct
    y = (torch.vstack(list_labels) - y_frozen).cpu().numpy()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 37.25 GiB (GPU 0; 4.00 GiB total capacity; 10.04 MiB already allocated; 3.20 GiB free; 24.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
